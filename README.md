ðŸ¤– AI Terraform Reviewer for Azure (Production-Grade)

An AI-assisted, deterministic Terraform Pull Request reviewer that analyzes Azure infrastructure changes, reasons about risk like a Staff Platform Engineer, and posts explainable, actionable feedback directly on GitHub Pull Requests.

This project intentionally combines:

Deterministic infrastructure analysis (source of truth)

Azure-aware domain knowledge

Historical memory

LLM-assisted explanations (Azure OpenAI)

Rules decide. LLM explains. Humans approve.

ðŸš€ Why This Project Exists

Most Terraform review tools suffer from one of these problems:

âŒ Only lint syntax (no real risk analysis)
âŒ Stateless (forget past incidents and PRs)
âŒ Over-trust LLMs (hallucinations, inconsistent risk)
âŒ Unsafe for production (non-deterministic decisions)

This project solves those problems by designing an AI governance system, not a chatbot.

ðŸ§  Core Design Principles

Determinism First

Same Terraform plan â†’ same risk â†’ every time

LLMs Are Not Decision Makers

LLMs only explain decisions already made

Explainability Over Cleverness

Every risk can be traced to a rule

Production Safety

LLM failure must never block CI

Human Authority

AI advises, humans decide

ðŸ—ï¸ High-Level Architecture
Pull Request
   â†“
GitHub Actions
   â†“
Terraform Plan (facts)
   â†“
Context Enrichment
   â†“
Deterministic Risk Engine
   â†“
Historical Memory Lookup
   â†“
LLM Explanation (Azure OpenAI)
   â†“
PR Comment (Explainable + Actionable)

ðŸ“‚ Repository Structure
ai-terraform-reviewer-azure/
â”‚
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ environments/
â”‚       â””â”€â”€ dev/                 # Sample Terraform environment
â”‚
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”œâ”€â”€ enrich.py             # Converts terraform plan â†’ enriched context
â”‚   â”‚   â”œâ”€â”€ tfplan.json           # Terraform plan (JSON)
â”‚   â”‚   â””â”€â”€ enriched_context.json # Classified infra context
â”‚   â”‚
â”‚   â”œâ”€â”€ reasoning/
â”‚   â”‚   â”œâ”€â”€ review.py             # Deterministic AI risk engine
â”‚   â”‚   â”œâ”€â”€ llm_enrichment.py     # Safe Azure OpenAI integration
â”‚   â”‚   â”œâ”€â”€ post_comment.py       # GitHub PR comment publisher
â”‚   â”‚   â””â”€â”€ ai_review.json        # Final AI review output
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ llm_client.py         # Azure OpenAI client (controlled)
â”‚   â”‚   â””â”€â”€ prompts.py            # Strict prompt contracts
â”‚   â”‚
â”‚   â””â”€â”€ memory/
â”‚       â”œâ”€â”€ memory_store.py       # Historical PR memory engine
â”‚       â””â”€â”€ pr_memory.json        # (Ignored) runtime memory store
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ terraform-ai-review.yml
â”‚
â””â”€â”€ README.md

ðŸ” What the AI Actually Does
Deterministic (Rules-Based)

The AI always detects:

Shared infrastructure changes

Azure networking changes (VNet, subnet)

Production vs non-production escalation

Large change sets (blast radius)

Repeated risky patterns (historical memory)

This logic lives in:

ai/reasoning/review.py

LLM-Assisted (Azure OpenAI)

The LLM is used only to:

Explain why a change is risky

Improve human readability

Maintain professional tone

The LLM:
âŒ Cannot change risk
âŒ Cannot block PRs
âŒ Cannot invent resources

If Azure OpenAI fails â†’ system still works.

ðŸ§  Example PR Comment (Real Output)
ðŸ¤– AI Terraform Review (Azure)

Environment: dev
Risk Level: HIGH
Confidence: 85%

Why this change is risky:
- Shared Azure infrastructure is being modified
- Azure networking resources are being modified

Recommended Actions:
- Run during maintenance window
- Ensure rollback plan
- Validate in lower environment

AI Reasoning (LLM-Assisted):
The proposed Terraform changes carry a high risk due to modifications
to shared Azure networking components...

ðŸ§  Why Not â€œOnly LLMâ€?

Using only an LLM would cause:

Non-reproducible decisions

Hallucinated risks

No audit trail

Loss of trust

This project uses a hybrid architecture:

Component	Responsibility
Rules	Decide risk
Memory	Learn from past
LLM	Explain clearly
Human	Approve

This is how real production systems work.

ðŸ” Security & Compliance

No secrets committed

Azure OpenAI keys stored in GitHub Secrets

LLM never sees codebase or credentials

All decisions are auditable

CI never fails due to AI

ðŸ§ª How to Run Locally (Learning Mode)
terraform init
terraform plan -out=tfplan
terraform show -json tfplan > tfplan.json

python ai/context/enrich.py tfplan.json enriched_context.json
python ai/reasoning/review.py enriched_context.json ai_review.json

ðŸ”„ How CI Works

On every Pull Request:

Terraform plan runs

Context is enriched

Deterministic risk is computed

LLM explanation is added (if available)

PR comment is posted

PR is recorded into historical memory